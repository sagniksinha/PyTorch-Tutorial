{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9403bfaf-e26d-4b7a-9f4f-09b674266d04",
   "metadata": {},
   "source": [
    "## Loss switching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13f392ac-3439-4815-b9fb-d462b4d44221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: alpha=1.00, Loss1=0.7756, Loss2=0.2785, Total=0.7756\n",
      "Epoch 2: alpha=0.67, Loss1=0.7737, Loss2=0.2778, Total=0.6084\n",
      "Epoch 3: alpha=0.33, Loss1=0.7718, Loss2=0.2770, Total=0.4419\n",
      "Epoch 4: alpha=0.00, Loss1=0.7700, Loss2=0.2763, Total=0.2763\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# =========================================================\n",
    "# 1) DUMMY MODEL\n",
    "# =========================================================\n",
    "# A simple linear model for demonstration purposes\n",
    "# Input: 10 features, Output: 2 classes\n",
    "model = nn.Linear(10, 2)\n",
    "\n",
    "# Optimizer: decides how to adjust model weights to reduce errors\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# =========================================================\n",
    "# 2) DUMMY DATA\n",
    "# =========================================================\n",
    "# Small batch of 32 samples, 10 features each\n",
    "x = torch.randn(32, 10)\n",
    "# Random integer labels for classification (0 or 1)\n",
    "y = torch.randint(0, 2, (32,))\n",
    "\n",
    "# =========================================================\n",
    "# 3) DEFINE TWO LOSSES\n",
    "# =========================================================\n",
    "# Loss1: CrossEntropyLoss for classification\n",
    "criterion1 = nn.CrossEntropyLoss()\n",
    "# Loss2: MSELoss as a second task/dummy loss\n",
    "criterion2 = nn.MSELoss()\n",
    "\n",
    "# Number of epochs for training\n",
    "NUM_EPOCHS = 4\n",
    "\n",
    "# =========================================================\n",
    "# 4) TRAINING LOOP WITH LOSS SWITCHING\n",
    "# =========================================================\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Step 4.1: Zero gradients from previous step\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Step 4.2: Forward pass through the model\n",
    "    logits = model(x)\n",
    "    \n",
    "    # Step 4.3: Compute both losses\n",
    "    # Loss1: standard classification loss\n",
    "    loss1 = criterion1(logits, y)\n",
    "    # Loss2: MSE between predicted probabilities and one-hot labels\n",
    "    y_onehot = torch.nn.functional.one_hot(y, num_classes=2).float()\n",
    "    loss2 = criterion2(torch.softmax(logits, dim=1), y_onehot)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Step 4.4: LOSS SWITCHING LOGIC\n",
    "    # -----------------------------\n",
    "    # Alpha factor controls contribution of each loss\n",
    "    # Start with 100% loss1 and 0% loss2\n",
    "    # End with 0% loss1 and 100% loss2\n",
    "    # Linear interpolation over epochs\n",
    "    alpha = 1 - (epoch / (NUM_EPOCHS - 1))  # 1 â†’ 0 across epochs\n",
    "    \n",
    "    # Combine losses according to alpha\n",
    "    total_loss = alpha * loss1 + (1 - alpha) * loss2\n",
    "    \n",
    "    # Step 4.5: Backpropagation\n",
    "    # Compute gradients for all model parameters\n",
    "    total_loss.backward()\n",
    "    \n",
    "    # Step 4.6: Update model weights based on gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Step 4.7: Print metrics\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}: \"\n",
    "        f\"alpha={alpha:.2f}, \"\n",
    "        f\"Loss1={loss1.item():.4f}, \"\n",
    "        f\"Loss2={loss2.item():.4f}, \"\n",
    "        f\"Total={total_loss.item():.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c575e707-fd1f-4262-8f36-120a587337cd",
   "metadata": {},
   "source": [
    "## Demo Of MoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c69167d1-ba9c-48d2-af5c-e7e0cac0cabb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mixture-of-Experts (MoE) demo combining two NLP experts (BERT + DistilBERT).\n",
    "\n",
    "- Uses pretrained models from Hugging Face when available (internet required).\n",
    "- Falls back to simple text models if transformers are unavailable or offline.\n",
    "- Uses a synthetic text dataset so it runs fully offline.\n",
    "- Demonstrates how the gating network learns to combine expert outputs.\n",
    "\n",
    "ðŸŸ© BERT excels at: Understanding nuanced language\n",
    "Great for semantic understanding, complex sentiment, entailment, question answering, and coreference resolution.\n",
    "Example: â€œThe movie was bad, but the acting was brilliant.â€\n",
    "â†’ BERT captures long-range context\n",
    "\n",
    "ðŸŸ¨ DistilBERT excels at: Speed and Efficiency\n",
    "~40% smaller, ~60% faster at inference.\n",
    "\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# =========================================================\n",
    "# 1) CONFIGURATION AND DEVICE SETUP\n",
    "# =========================================================\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "NUM_CLASSES = 10\n",
    "EMBED_DIM = 256\n",
    "NUM_EPOCHS = 1\n",
    "USE_PRETRAINED = True\n",
    "\n",
    "# =========================================================\n",
    "# 2) SYNTHETIC TEXT DATASET\n",
    "# =========================================================\n",
    "class SyntheticSentimentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates short, meaningful sentences with positive or negative sentiment.\n",
    "    Each sample: (sentence, label)\n",
    "    Label: 1 = Positive, 0 = Negative\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n=512):\n",
    "        self.n = n\n",
    "\n",
    "        # Define word pools\n",
    "        self.positive_subjects = [\"I\", \"We\", \"My friend\", \"Our team\", \"The movie\"]\n",
    "        self.negative_subjects = [\"I\", \"We\", \"My boss\", \"The service\", \"The food\"]\n",
    "\n",
    "        self.positive_verbs = [\"love\", \"enjoy\", \"like\", \"appreciate\", \"recommend\"]\n",
    "        self.negative_verbs = [\"hate\", \"dislike\", \"regret\", \"complain about\", \"avoid\"]\n",
    "\n",
    "        self.positive_objects = [\"the product\", \"this place\", \"the performance\",\n",
    "                                 \"the design\", \"the food\", \"the experience\"]\n",
    "        self.negative_objects = [\"the delay\", \"the taste\", \"this service\",\n",
    "                                 \"the experience\", \"the product\", \"the noise\"]\n",
    "\n",
    "        self.positive_endings = [\"It was amazing!\", \"Really great.\", \"Highly recommended!\",\n",
    "                                 \"Would come again.\", \"Such a good feeling.\"]\n",
    "        self.negative_endings = [\"It was terrible.\", \"Really bad.\", \"Not worth it.\",\n",
    "                                 \"Never again.\", \"Disappointing experience.\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Randomly choose sentiment\n",
    "        label = random.choice([0, 1])  # 0 = negative, 1 = positive\n",
    "\n",
    "        if label == 1:\n",
    "            subj = random.choice(self.positive_subjects)\n",
    "            verb = random.choice(self.positive_verbs)\n",
    "            obj = random.choice(self.positive_objects)\n",
    "            end = random.choice(self.positive_endings)\n",
    "        else:\n",
    "            subj = random.choice(self.negative_subjects)\n",
    "            verb = random.choice(self.negative_verbs)\n",
    "            obj = random.choice(self.negative_objects)\n",
    "            end = random.choice(self.negative_endings)\n",
    "\n",
    "        # Create a simple but meaningful sentence\n",
    "        text = f\"{subj} {verb} {obj}. {end}\"\n",
    "\n",
    "        return text, label\n",
    "\n",
    "\n",
    "# Instantiate dataset and dataloader\n",
    "dataset = SyntheticSentimentDataset(n=512)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55951a0c-557a-448e-b989-95ef97f607f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample records from SyntheticTextDataset:\n",
      " 1) Text: My boss avoid the delay. It was terrible.\n",
      "    Label: 0\n",
      "\n",
      " 2) Text: Our team love this place. Such a good feeling.\n",
      "    Label: 1\n",
      "\n",
      " 3) Text: My friend appreciate this place. Highly recommended!\n",
      "    Label: 1\n",
      "\n",
      " 4) Text: I like the product. Would come again.\n",
      "    Label: 1\n",
      "\n",
      " 5) Text: The movie enjoy the design. It was amazing!\n",
      "    Label: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSample records from SyntheticTextDataset:\")\n",
    "for i in range(5):  # show 5 random samples\n",
    "    text, label = dataset[i]\n",
    "    print(f\"{i+1:>2}) Text: {text}\")\n",
    "    print(f\"    Label: {label}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca2e688a-d72d-456c-924c-7aa983bd4ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9767d7dcc9b4e78a20eeab65baf2779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagni\\anaconda3\\envs\\AgenticAI\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sagni\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aeece5f20a542c2a448930b6af3fd30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e330b16309481281758a0680a28424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976244e0000248859d9653d35b31c315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 3) TOKENIZER SETUP (uses Hugging Face if available)\n",
    "# =========================================================\n",
    "\n",
    "try:\n",
    "    # Attempt to import Hugging Face transformers library\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    HF_AVAILABLE = True  # Flag indicating that HF tokenizer/model can be used\n",
    "\n",
    "    # Initialize BERT tokenizer\n",
    "    tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    # Initialize DistilBERT tokenizer\n",
    "    tokenizer_distilbert = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "except Exception:\n",
    "    # If transformers library or pretrained models are not available\n",
    "    HF_AVAILABLE = False  # Fall back to simple tokenizer\n",
    "    tokenizer_bert = tokenizer_distilbert = None\n",
    "    print(\"Transformers not available; falling back to simple tokenizer.\")\n",
    "\n",
    "\n",
    "def simple_tokenize(batch_texts, max_len=16, tokenizer=None):\n",
    "    \"\"\"\n",
    "    Tokenize a batch of text strings into input IDs and attention masks.\n",
    "    \n",
    "    Args:\n",
    "        batch_texts: list of text strings\n",
    "        max_len: maximum sequence length (truncate/pad sentences)\n",
    "        tokenizer: optional Hugging Face tokenizer to use\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with:\n",
    "            'input_ids': tensor of token IDs [batch_size, max_len]\n",
    "            'attention_mask': tensor of 0/1 mask [batch_size, max_len]\n",
    "    \"\"\"\n",
    "\n",
    "    if HF_AVAILABLE and tokenizer is not None:\n",
    "        # Use Hugging Face tokenizer if available\n",
    "        enc = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=\"max_length\",       # pad sentences to max_len\n",
    "            truncation=True,            # truncate sentences longer than max_len\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\"         # return PyTorch tensors\n",
    "        )\n",
    "        return {\"input_ids\": enc[\"input_ids\"], \"attention_mask\": enc[\"attention_mask\"]}\n",
    "\n",
    "    else:\n",
    "        # Fallback: simple tokenization if HF tokenizer is unavailable\n",
    "        input_ids, attention_mask = [], []\n",
    "\n",
    "        for t in batch_texts:\n",
    "            # Split sentence into words and truncate to max_len\n",
    "            toks = t.split()[:max_len]\n",
    "\n",
    "            # Convert words to small integer IDs via hashing\n",
    "            ids = [(abs(hash(w)) % 1000) + 1 for w in toks]\n",
    "\n",
    "            # Pad sequence to max_len with 0s\n",
    "            pad_len = max_len - len(ids)\n",
    "            ids = ids + [0]*pad_len\n",
    "\n",
    "            # Create attention mask: 1 for real tokens, 0 for padding\n",
    "            mask = [1]*len(toks) + [0]*pad_len\n",
    "\n",
    "            input_ids.append(ids)\n",
    "            attention_mask.append(mask)\n",
    "\n",
    "        # Convert lists to PyTorch tensors\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e08c26e-f13c-44c2-be76-68a5fc940454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 4) NLP EXPERT MODELS (BERT + DISTILBERT)\n",
    "# =========================================================\n",
    "class NLPExpert(nn.Module):\n",
    "    \"\"\"\n",
    "    This class represents a single NLP â€œexpertâ€ in our mixture-of-experts setup.\n",
    "    Conceptually, it takes a piece of text and turns it into a fixed-size numerical vector \n",
    "    (embedding) that summarizes the meaning of the sentence.\n",
    "\n",
    "    - If a pretrained transformer (like BERT or DistilBERT) is available, it uses it \n",
    "      to get a sophisticated understanding of the text.\n",
    "    - If transformers are not available (offline or demo mode), it uses a small simple model \n",
    "      that still produces a vector representation from the words.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", out_dim=EMBED_DIM):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "\n",
    "        try:\n",
    "            # Try to load a pretrained transformer for rich language understanding\n",
    "            self.transformer = AutoModel.from_pretrained(model_name)\n",
    "            # A small linear layer converts transformer output to the size we need for our model\n",
    "            self.fc = nn.Linear(self.transformer.config.hidden_size, out_dim)\n",
    "            self._is_pretrained = True  # remember that we are using a real transformer\n",
    "        except Exception:\n",
    "            # Fallback: lightweight model for when transformers aren't available\n",
    "            # Embedding layer converts words to vectors\n",
    "            self.embedding = nn.Embedding(1001, 128, padding_idx=0)\n",
    "            # Simple pooling over words to get one vector representing the whole sentence\n",
    "            self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "            # Linear layer maps the pooled vector to the size our MoE model expects\n",
    "            self.fc = nn.Linear(128, out_dim)\n",
    "            self._is_pretrained = False  # using simple fallback\n",
    "\n",
    "    def forward(self, text_inputs):\n",
    "        \"\"\"\n",
    "        Convert a batch of text into a batch of fixed-size vectors.\n",
    "\n",
    "        Input:\n",
    "            text_inputs: a dictionary containing tokenized text\n",
    "                         ('input_ids' and 'attention_mask')\n",
    "        Output:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0061ef0-1846-4a3c-a55d-658e7a33019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 5) GATING NETWORK â€” decides expert weights per sample\n",
    "# =========================================================\n",
    "class SimpleGate(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the â€œdecision makerâ€ of the Mixture-of-Experts model.\n",
    "\n",
    "    Conceptually:\n",
    "    - The gate looks at a summary vector representing the input (context_feat).\n",
    "    - It decides **how much to trust each expert** for this particular input.\n",
    "    - Outputs a set of weights (one per expert) that sum to 1.\n",
    "      These weights will later be used to combine expert outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=EMBED_DIM, num_experts=2):\n",
    "        super().__init__()\n",
    "        # Linear layer maps input context vector to one score per expert\n",
    "        self.fc = nn.Linear(input_dim, num_experts)\n",
    "\n",
    "    def forward(self, context_feat):\n",
    "        \"\"\"\n",
    "        Forward pass:\n",
    "        1. Take a context vector representing the input.\n",
    "        2. Produce raw scores for each expert using a linear layer.\n",
    "        3. Convert scores to probabilities using softmax (so they sum to 1).\n",
    "        4. Return these probabilities (weights) for combining experts.\n",
    "\n",
    "        Input:\n",
    "            context_feat: [batch_size, input_dim] summary of the input\n",
    "        Output:\n",
    "            weights: [batch_size, num_experts] probabilities for each expert\n",
    "        \"\"\"\n",
    "        logits = self.fc(context_feat)         # Raw score per expert\n",
    "        weights = F.softmax(logits, dim=-1)    # Convert to probabilities\n",
    "        return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "857933d2-1c38-43cc-832a-0230b93f809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 6) MIXTURE OF EXPERTS (COMBINES BERT + DISTILBERT)\n",
    "# =========================================================\n",
    "class TextMoE(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements the Mixture-of-Experts (MoE) model for text.\n",
    "    \n",
    "    Conceptually:\n",
    "    - We have multiple NLP â€œexpertsâ€ (e.g., BERT and DistilBERT).\n",
    "    - Each expert produces its own vector representation of the input sentence.\n",
    "    - A gating network decides how much to trust each expert for this particular input.\n",
    "    - The expert outputs are then combined (weighted sum) according to the gate.\n",
    "    - The combined vector is passed to a final classifier to predict the label.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, expert1, expert2, gate, out_classes=NUM_CLASSES):\n",
    "        super().__init__()\n",
    "        # Store the two experts in a ModuleList\n",
    "        self.experts = nn.ModuleList([expert1, expert2])\n",
    "        # The gating network decides per-input expert weights\n",
    "        self.gate = gate\n",
    "        # Final classification layer maps combined embedding to class probabilities\n",
    "        self.output_head = nn.Linear(EMBED_DIM, out_classes)\n",
    "\n",
    "    def forward(self, tokenized1, tokenized2):\n",
    "        \"\"\"\n",
    "        Forward pass for the MoE model.\n",
    "\n",
    "        Inputs:\n",
    "            tokenized1: tokenized text for expert1 (e.g., BERT)\n",
    "            tokenized2: tokenized text for expert2 (e.g., DistilBERT)\n",
    "\n",
    "        Steps:\n",
    "        1. Get embeddings from each expert separately\n",
    "            - out1 = expert1(tokenized1)\n",
    "            - out2 = expert2(tokenized2)\n",
    "        2. Create a â€œcontext vectorâ€ for the gate\n",
    "            - Here, we simply take the average of the expert embeddings\n",
    "        3. Compute soft weights for each expert using the gating network\n",
    "        4. Stack expert embeddings together\n",
    "        5. Multiply each expert embedding by its weight and sum\n",
    "           â†’ produces a single combined vector representing the input\n",
    "        6. Pass the combined vector through a classification layer\n",
    "           â†’ produces final logits for each class\n",
    "        7. Return both logits and the gate weights (useful for inspection)\n",
    "        \"\"\"\n",
    "        # Step 1: forward through both experts\n",
    "        out1 = self.experts[0](tokenized1)   # BERT output\n",
    "        out2 = self.experts[1](tokenized2)   # DistilBERT output\n",
    "\n",
    "        # Step 2: compute context for gating\n",
    "        context = (out1 + out2) / 2\n",
    "\n",
    "        # Step 3: gate decides weights for each expert\n",
    "        weights = self.gate(context)  # [batch_size, 2]\n",
    "\n",
    "        # Step 4: stack expert embeddings for weighted combination\n",
    "        stacked = torch.stack([out1, out2], dim=-1)  # [B, EMBED_DIM, 2]\n",
    "\n",
    "        # Step 5: combine experts according to weights\n",
    "        mixed = (stacked * weights.unsqueeze(1)).sum(dim=-1)\n",
    "\n",
    "        # Step 6: final classification\n",
    "        logits = self.output_head(mixed)\n",
    "\n",
    "        # Step 7: return logits and gate weights\n",
    "        return logits, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5ea7c11-8522-4e1a-9db1-0e1349e73032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08052183fcb42c59d0c95cc3d6d12e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created.\n",
      "Expert1: bert-base-uncased, pretrained: True\n",
      "Expert2: distilbert-base-uncased, pretrained: True\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 7) MODEL INITIALIZATION\n",
    "# =========================================================\n",
    "expert1 = NLPExpert(model_name=\"bert-base-uncased\", out_dim=EMBED_DIM)\n",
    "expert2 = NLPExpert(model_name=\"distilbert-base-uncased\", out_dim=EMBED_DIM)\n",
    "gate = SimpleGate(input_dim=EMBED_DIM, num_experts=2)\n",
    "\n",
    "model = TextMoE(expert1, expert2, gate).to(device)\n",
    "print(\"Model created.\")\n",
    "print(f\"Expert1: {expert1.model_name}, pretrained: {expert1._is_pretrained}\")\n",
    "print(f\"Expert2: {expert2.model_name}, pretrained: {expert2._is_pretrained}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1a3ac3b-51a6-4f50-8e85-85416ed2fb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:03<00:00,  9.41it/s, loss=2.3116, acc=0.094, w1=0.510, w2=0.490]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=2.3116, Acc=0.0938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 8) OPTIMIZER / LOSS FUNCTION\n",
    "# =========================================================\n",
    "# Optimizer: decides how to adjust model weights to reduce errors\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Loss function: measures how wrong the model's predictions are\n",
    "# CrossEntropyLoss is used for multi-class classification tasks\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 9) TRAINING LOOP\n",
    "# =========================================================\n",
    "# We train the model for NUM_EPOCHS passes over the dataset\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()  # set model to training mode\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "\n",
    "    for texts, labels in pbar:\n",
    "        # Step 1: Tokenize text for both experts\n",
    "        tokenized1 = simple_tokenize(texts, tokenizer=tokenizer_bert)\n",
    "        tokenized2 = simple_tokenize(texts, tokenizer=tokenizer_distilbert)\n",
    "\n",
    "        # Move all inputs to the correct device (CPU/GPU)\n",
    "        for k in tokenized1: tokenized1[k] = tokenized1[k].to(device)\n",
    "        for k in tokenized2: tokenized2[k] = tokenized2[k].to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Step 2: Zero gradients from previous step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Step 3: Forward pass through the Mixture-of-Experts model\n",
    "        # Returns:\n",
    "        #   logits = predicted class scores\n",
    "        #   gate_w = weights assigned to each expert for this batch\n",
    "        logits, gate_w = model(tokenized1, tokenized2)\n",
    "\n",
    "        # Step 4: Compute loss between predictions and true labels\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Step 5: Backpropagation â€” compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Step 6: Update model weights based on gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Step 7: Track running metrics\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        _, predicted = logits.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        # Step 8: Update progress bar with loss, accuracy, and gate weights\n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{running_loss/total:.4f}\",\n",
    "            \"acc\": f\"{correct/total:.3f}\",\n",
    "            \"w1\": f\"{gate_w[:,0].mean().item():.3f}\",  # average weight for expert1\n",
    "            \"w2\": f\"{gate_w[:,1].mean().item():.3f}\"   # average weight for expert2\n",
    "        })\n",
    "\n",
    "    # Step 9: Compute and print epoch-level metrics\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = correct / total\n",
    "    print(f\"Epoch {epoch+1}: Loss={epoch_loss:.4f}, Acc={epoch_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
